train_implicit_upsampler.py:65: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs", config_name="implicit_upsampler.yaml")
/home/t/tanriverdi/.local/lib/python3.8/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
Global seed set to 0
  0%|          | 0/47 [00:00<?, ?it/s]/home/t/tanriverdi/.local/lib/python3.8/site-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/t/tanriverdi/.local/lib/python3.8/site-packages/torch/nn/functional.py:3502: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
  2%|▏         | 1/47 [00:00<00:09,  4.70it/s]  4%|▍         | 2/47 [00:00<00:08,  5.04it/s]  6%|▋         | 3/47 [00:00<00:08,  5.29it/s]  9%|▊         | 4/47 [00:00<00:07,  5.42it/s] 11%|█         | 5/47 [00:00<00:07,  5.50it/s] 13%|█▎        | 6/47 [00:01<00:07,  5.54it/s] 15%|█▍        | 7/47 [00:01<00:07,  5.56it/s] 17%|█▋        | 8/47 [00:01<00:07,  5.52it/s] 19%|█▉        | 9/47 [00:01<00:06,  5.50it/s] 21%|██▏       | 10/47 [00:01<00:06,  5.48it/s] 23%|██▎       | 11/47 [00:02<00:06,  5.47it/s] 26%|██▌       | 12/47 [00:02<00:06,  5.45it/s] 28%|██▊       | 13/47 [00:02<00:06,  5.44it/s] 30%|██▉       | 14/47 [00:02<00:06,  5.44it/s] 32%|███▏      | 15/47 [00:02<00:05,  5.47it/s] 34%|███▍      | 16/47 [00:02<00:05,  5.45it/s] 36%|███▌      | 17/47 [00:03<00:05,  5.44it/s] 38%|███▊      | 18/47 [00:03<00:05,  5.43it/s] 40%|████      | 19/47 [00:03<00:05,  5.43it/s] 43%|████▎     | 20/47 [00:03<00:04,  5.42it/s] 45%|████▍     | 21/47 [00:03<00:04,  5.42it/s] 47%|████▋     | 22/47 [00:04<00:04,  5.42it/s] 49%|████▉     | 23/47 [00:04<00:04,  5.41it/s] 51%|█████     | 24/47 [00:04<00:04,  5.41it/s] 53%|█████▎    | 25/47 [00:04<00:04,  5.39it/s] 55%|█████▌    | 26/47 [00:04<00:03,  5.39it/s] 57%|█████▋    | 27/47 [00:04<00:03,  5.40it/s] 60%|█████▉    | 28/47 [00:05<00:03,  5.40it/s] 62%|██████▏   | 29/47 [00:05<00:03,  5.41it/s] 64%|██████▍   | 30/47 [00:05<00:03,  5.41it/s] 66%|██████▌   | 31/47 [00:05<00:02,  5.41it/s] 68%|██████▊   | 32/47 [00:05<00:02,  5.41it/s] 70%|███████   | 33/47 [00:06<00:02,  5.41it/s] 72%|███████▏  | 34/47 [00:06<00:02,  5.41it/s] 74%|███████▍  | 35/47 [00:06<00:02,  5.42it/s] 77%|███████▋  | 36/47 [00:06<00:02,  5.41it/s] 79%|███████▊  | 37/47 [00:06<00:01,  5.41it/s] 81%|████████  | 38/47 [00:07<00:01,  5.42it/s] 83%|████████▎ | 39/47 [00:07<00:01,  5.41it/s] 85%|████████▌ | 40/47 [00:07<00:01,  5.41it/s] 87%|████████▋ | 41/47 [00:07<00:01,  5.35it/s] 89%|████████▉ | 42/47 [00:07<00:00,  5.34it/s] 91%|█████████▏| 43/47 [00:07<00:00,  5.32it/s] 94%|█████████▎| 44/47 [00:08<00:00,  5.34it/s] 96%|█████████▌| 45/47 [00:08<00:00,  5.36it/s] 98%|█████████▊| 46/47 [00:08<00:00,  5.37it/s]100%|██████████| 47/47 [00:08<00:00,  5.57it/s]100%|██████████| 47/47 [00:08<00:00,  5.42it/s]
/home/t/tanriverdi/Desktop/FeatUp/featup/util.py:234: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(sklearn_pca.components_, device=device, dtype=feats.dtype))
/home/t/tanriverdi/Desktop/FeatUp/featup/util.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(sklearn_pca.singular_values_, device=device, dtype=feats.dtype))
/home/t/tanriverdi/Desktop/FeatUp/featup/util.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.register_buffer('mean_', torch.tensor(sklearn_pca.mean_, device=device, dtype=feats.dtype))
Image 0 of 1:   0%|          | 0/1200 [00:00<?, ?it/s]Image 0 of 1:   0%|          | 0/1200 [00:00<?, ?it/s]
Error executing job with overrides: []
Traceback (most recent call last):
  File "train_implicit_upsampler.py", line 362, in my_app
    writer.add_histogram("scales hist", scales, step)
  File "/home/t/tanriverdi/.local/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py", line 429, in add_histogram
    histogram(tag, values, bins, max_bins=max_bins), global_step, walltime)
  File "/home/t/tanriverdi/.local/lib/python3.8/site-packages/torch/utils/tensorboard/summary.py", line 300, in histogram
    hist = make_histogram(values.astype(float), bins, max_bins)
  File "/home/t/tanriverdi/.local/lib/python3.8/site-packages/torch/utils/tensorboard/summary.py", line 324, in make_histogram
    cum_counts = np.cumsum(np.greater(counts, 0, dtype=np.int32))
TypeError: No loop matching the specified signature and casting was found for ufunc greater

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
